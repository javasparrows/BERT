{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://qiita.com/sugulu_Ogawa_ISID/items/697bd03499c1de9cf082\n",
    "を参考にさせていただきました"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Livedoorニュースのファイルをダウンロード\n",
    "# !wget \"https://www.rondhuit.com/download/ldcc-20140209.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dokujo-tsushin', 'kaden-channel', 'it-life-hack', 'topic-news', 'smax', 'README.txt', 'livedoor-homme', 'sports-watch', 'peachy', 'movie-enter', 'CHANGES.txt']\n",
      "カテゴリー数: 9\n",
      "['dokujo-tsushin', 'kaden-channel', 'it-life-hack', 'topic-news', 'smax', 'livedoor-homme', 'sports-watch', 'peachy', 'movie-enter']\n"
     ]
    }
   ],
   "source": [
    "# ファイルを解凍し、カテゴリー数と内容を確認\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "# 解凍\n",
    "# tar = tarfile.open(\"ldcc-20140209.tar.gz\", \"r:gz\")\n",
    "# tar.extractall(\"./data/livedoor/\")\n",
    "# tar.close()\n",
    "\n",
    "# フォルダのファイルとディレクトリを確認\n",
    "files_folders = [name for name in os.listdir(\"./data/livedoor/text/\")]\n",
    "print(files_folders)\n",
    "\n",
    "# カテゴリーのフォルダのみを抽出\n",
    "categories = [name for name in os.listdir(\n",
    "    \"./data/livedoor/text/\") if os.path.isdir(\"./data/livedoor/text/\"+name)]\n",
    "\n",
    "print(\"カテゴリー数:\", len(categories))\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0： http://news.livedoor.com/article/detail/6255260/\n",
      "\n",
      "1： 2012-02-07T09:00:00+0900\n",
      "\n",
      "2： 新しいヴァンパイアが誕生！　ジョニデ主演『ダーク・シャドウ』の公開日が決定\n",
      "\n",
      "3： 　こんなヴァンパイアは見たことがない！　ジョニー・デップとティム・バートン監督がタッグを組んだ映画『ダーク・シャドウズ（原題）』の邦題が『ダーク・シャドウ』に決定。日本公開日が5月19日に決まった。さらに、ジョニー・デップ演じるヴァンパイアの写真が公開された。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ファイルの中身を確認してみる\n",
    "file_name = \"./data/livedoor/text/movie-enter/movie-enter-6255260.txt\"\n",
    "\n",
    "with open(file_name) as text_file:\n",
    "    text = text_file.readlines()\n",
    "    print(\"0：\", text[0])  # URL情報\n",
    "    print(\"1：\", text[1])  # タイムスタンプ\n",
    "    print(\"2：\", text[2])  # タイトル\n",
    "    print(\"3：\", text[3])  # 本文\n",
    "\n",
    "    # 今回は4要素目には本文は伸びていないが、4要素目以降に本文がある場合もある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本文を取得する前処理関数を定義\n",
    "\n",
    "def extract_main_txt(file_name):\n",
    "    with open(file_name) as text_file:\n",
    "        # 今回はタイトル行は外したいので、3要素目以降の本文のみ使用\n",
    "        text = text_file.readlines()[3:]\n",
    "\n",
    "        # 3要素目以降にも本文が入っている場合があるので、リストにして、後で結合させる\n",
    "        text = [sentence.strip() for sentence in text]  # 空白文字(スペースやタブ、改行)の削除\n",
    "        text = list(filter(lambda line: line != '', text))\n",
    "        text = ''.join(text)\n",
    "        text = text.translate(str.maketrans(\n",
    "            {'\\n': '', '\\t': '', '\\r': '', '\\u3000': ''}))  # 改行やタブ、全角スペースを消す\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# リストに前処理した本文と、カテゴリーのラベルを追加していく\n",
    "import glob\n",
    "\n",
    "list_text = []\n",
    "list_label = []\n",
    "\n",
    "for cat in categories:\n",
    "    text_files = glob.glob(os.path.join(\"./data/livedoor/text\", cat, \"*.txt\"))\n",
    "\n",
    "    # 前処理extract_main_txtを実施して本文を取得\n",
    "    body = [extract_main_txt(text_file) for text_file in text_files]\n",
    "\n",
    "    label = [cat] * len(body)  # bodyの数文だけカテゴリー名のラベルのリストを作成\n",
    "\n",
    "    list_text.extend(body)  # appendが要素を追加するのに対して、extendはリストごと追加する\n",
    "    list_label.extend(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7376\n",
      "7376\n"
     ]
    }
   ],
   "source": [
    "print(len(list_text))\n",
    "print(len(list_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今年もクリスマスが近い。楽しみでもあり頭を悩ませるのは大好きな人へのプレゼント選びだ。何をあげたら喜んでくれるか、何をもらったら嬉しいかは、恋愛年数や2人の親密度によっても異なるだろうが、一緒に暮らしているカップルはどんなプレゼントが嬉しいのか、結婚歴10年以上の夫婦に聞いてみた。「出所は一緒なので、誕生日などはその時ほしいものがあれば買ってもらうし、クリスマスでもこれといってプレゼント交換もしないんですけど」と言いながら、夫から一度だけサプライズ的なプレゼントをもらったことがあるという結婚歴11年のマコさん（39歳）。品物は某メーカーの基礎化粧品。「使ってみたくても高くてなかなか手が出せない商品だったので、いつもテレビCMを見ては「これってお肌にいいんだってさ」とぼやいていたんです。でもまさか夫がそれを買ってくれるとは」ものすごく嬉しかったが、使い終われば、またいつもの安価な基礎化粧品に戻すはめに。それでもわざわざ化粧品売り場まで買いにいってくれた夫には感動したそうだ。結婚歴20年。二人の子供がいる桃子さん（45歳）の一番嬉しかったクリスマスプレゼントは『エマ』というコミック全巻。朝起きたら、子ども用のプレゼントと一緒にソファの上に。思わず飛び上がったそうだ。桃子さんから夫へのプレゼントはいつもカジュアルな洋服。「夫は自分で服を買わない人なので、私好みの服で私好みの男にしています」と幸せそうな笑みを浮かべる桃子さんは、クリスマス以外で夫からもらった最高に嬉しかったプレゼントがあるという。「高いものではないんですけど、私の好みのデザインで、今もずっと愛用しています」品物は腕時計。7年前に義父が亡くなったときに、葬儀やその後の事務的なことも含め桃子さんが頑張ったことへのねぎらいとして夫から贈られたものだ。「実は義父の葬儀は気苦労も多く、自分的には大変な葬儀だったので、それを夫が理解してくれていたことが分かって本当に嬉しかったんです」感謝やねぎらいも形にしないと伝わらない場合が多い。記念日以外のこうしたプレゼントは、夫婦に限らず、人間関係を円滑にする手段にもなりそうだ。結婚歴27年の洋子さん（50歳）にも10年以上愛用している夫からのプレゼントがある。モンブランの万年筆だ。万年筆は書き癖があるので夫と一緒に選んだそうだが、その時、夫がこっそり名前入りの手帳を注文してくれていて、思いがけない嬉しいサプライズになったという。夫婦っていいなと思う話ばかりが続くが、結婚歴38年。独身時代は写真が趣味だった義男さん（62歳）は、小遣いをためて憧れの一眼レフ「ペンタックスII」を買った。一番グレードの低いカメラだったが、ファインダーを覗いた時の興奮は今も覚えているという。その後、結婚して奥さんから同じ「ペンタックス」のSPという一番高級グレードのカメラをプレゼントしてもらったそうだ。「嬉しいなんてものではありませんでしたよ」そのカメラがきっかけで、義男さんはカメラの仕事を始め、現在はプロカメラマンとして活躍している。「好きなことが仕事になったのも妻のおかげです」とにっこり。夫婦は一緒に生活をしているから、相手が何を欲しいかは分かりやすいだろう。とはいっても相手に関心がなければ欲しいものなど分からない。お互いが何を欲しがっているのかが瞬時に分かるのが、理想の夫婦には違いないが、基礎化粧品のテレビCMを見ては「これいいんだってさ〜」とぼやく妻のために、化粧品売り場に足を運ぶのは、相手を思う心がなければできないことだ。コミック全巻も万年筆もカメラも、相手を大切に思う心がなければ選べない品物ばかりだ。クリスマスが近づくと、プレゼント人気ランキングとか品物にばかり関心がいくか、一番大切なのは相手を思う心であり、高価な品物より、あなたにしかできない心遣いが相手に伝わることが、最高のプレゼントになるのではないだろうか。（オフィスエムツー／佐枝せつこ）\n",
      "\n",
      "dokujo-tsushin\n"
     ]
    }
   ],
   "source": [
    "# 0番目の文章とラベルを確認\n",
    "print(list_text[0])\n",
    "print()\n",
    "print(list_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7376, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>今年もクリスマスが近い。楽しみでもあり頭を悩ませるのは大好きな人へのプレゼント選びだ。何をあ...</td>\n",
       "      <td>dokujo-tsushin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>女の子の節句「ひな祭り」。バレンタインデー、ホワイトデーよりも忘れられがちだが、立派な日本の...</td>\n",
       "      <td>dokujo-tsushin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>『あなたはどんな人物？』『何を目指している？』『あなたにとって一番大切なものは何？』庶民であ...</td>\n",
       "      <td>dokujo-tsushin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>日本人の死因のトップである、がん。ひとことで言えば悪性の腫瘍で、いろんな種類のがんがあるんだ...</td>\n",
       "      <td>dokujo-tsushin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>独女編集部、独女代表Mです。韓国では、天安艦事件が起こり物々しい雰囲気に包まれていますが、日...</td>\n",
       "      <td>dokujo-tsushin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text           label\n",
       "0  今年もクリスマスが近い。楽しみでもあり頭を悩ませるのは大好きな人へのプレゼント選びだ。何をあ...  dokujo-tsushin\n",
       "1  女の子の節句「ひな祭り」。バレンタインデー、ホワイトデーよりも忘れられがちだが、立派な日本の...  dokujo-tsushin\n",
       "2  『あなたはどんな人物？』『何を目指している？』『あなたにとって一番大切なものは何？』庶民であ...  dokujo-tsushin\n",
       "3  日本人の死因のトップである、がん。ひとことで言えば悪性の腫瘍で、いろんな種類のがんがあるんだ...  dokujo-tsushin\n",
       "4  独女編集部、独女代表Mです。韓国では、天安艦事件が起こり物々しい雰囲気に包まれていますが、日...  dokujo-tsushin"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandasのDataFrameにする\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'text': list_text, 'label': list_label})\n",
    "\n",
    "# 大きさを確認しておく（7,376文章が存在）\n",
    "print(df.shape)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'dokujo-tsushin', 1: 'kaden-channel', 2: 'it-life-hack', 3: 'topic-news', 4: 'smax', 5: 'livedoor-homme', 6: 'sports-watch', 7: 'peachy', 8: 'movie-enter'}\n",
      "{'dokujo-tsushin': 0, 'kaden-channel': 1, 'it-life-hack': 2, 'topic-news': 3, 'smax': 4, 'livedoor-homme': 5, 'sports-watch': 6, 'peachy': 7, 'movie-enter': 8}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>今年もクリスマスが近い。楽しみでもあり頭を悩ませるのは大好きな人へのプレゼント選びだ。何をあ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>女の子の節句「ひな祭り」。バレンタインデー、ホワイトデーよりも忘れられがちだが、立派な日本の...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>『あなたはどんな人物？』『何を目指している？』『あなたにとって一番大切なものは何？』庶民であ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>日本人の死因のトップである、がん。ひとことで言えば悪性の腫瘍で、いろんな種類のがんがあるんだ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>独女編集部、独女代表Mです。韓国では、天安艦事件が起こり物々しい雰囲気に包まれていますが、日...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label_index\n",
       "0  今年もクリスマスが近い。楽しみでもあり頭を悩ませるのは大好きな人へのプレゼント選びだ。何をあ...            0\n",
       "1  女の子の節句「ひな祭り」。バレンタインデー、ホワイトデーよりも忘れられがちだが、立派な日本の...            0\n",
       "2  『あなたはどんな人物？』『何を目指している？』『あなたにとって一番大切なものは何？』庶民であ...            0\n",
       "3  日本人の死因のトップである、がん。ひとことで言えば悪性の腫瘍で、いろんな種類のがんがあるんだ...            0\n",
       "4  独女編集部、独女代表Mです。韓国では、天安艦事件が起こり物々しい雰囲気に包まれていますが、日...            0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# カテゴリーの辞書を作成\n",
    "dic_id2cat = dict(zip(list(range(len(categories))), categories))\n",
    "dic_cat2id = dict(zip(categories, list(range(len(categories)))))\n",
    "\n",
    "print(dic_id2cat)\n",
    "print(dic_cat2id)\n",
    "\n",
    "# DataFrameにカテゴリーindexの列を作成\n",
    "df[\"label_index\"] = df[\"label\"].map(dic_cat2id)\n",
    "df.head()\n",
    "\n",
    "# label列を消去し、text, indexの順番にする\n",
    "df = df.loc[:, [\"text\", \"label_index\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U-20女子ワールドカップも後半に入り、「ヤングなでしこ」こと日本のU-20女子代表チームの...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>時に熱い言葉を発し、時に野球に関する議論を交わしながらも、時にはファンにしっかりと反論する。...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5日放送「ダウンタウンDX」（読売テレビ）には、ハロー！プロジェクトのプロデューサー＝つんく...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>フジテレビ「とくダネ！」（1日放送分）には、アジアカップを制した日本代表の岩政大樹、伊野波雅...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MEDIAS X N-07Dの画面キャプチャの方法を紹介！本日13日（金）にNTTドコモから...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label_index\n",
       "0  U-20女子ワールドカップも後半に入り、「ヤングなでしこ」こと日本のU-20女子代表チームの...            6\n",
       "1  時に熱い言葉を発し、時に野球に関する議論を交わしながらも、時にはファンにしっかりと反論する。...            6\n",
       "2  5日放送「ダウンタウンDX」（読売テレビ）には、ハロー！プロジェクトのプロデューサー＝つんく...            3\n",
       "3  フジテレビ「とくダネ！」（1日放送分）には、アジアカップを制した日本代表の岩政大樹、伊野波雅...            6\n",
       "4  MEDIAS X N-07Dの画面キャプチャの方法を紹介！本日13日（金）にNTTドコモから...            4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 順番をシャッフルする\n",
    "df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1475, 2)\n",
      "(5901, 2)\n"
     ]
    }
   ],
   "source": [
    "# tsvファイルで保存する\n",
    "\n",
    "# 全体の2割の文章数\n",
    "len_0_2 = len(df) // 5\n",
    "\n",
    "# 前から2割をテストデータとする\n",
    "df[:len_0_2].to_csv(\"./test.tsv\", sep='\\t', index=False, header=None)\n",
    "print(df[:len_0_2].shape)\n",
    "\n",
    "# 前2割からを訓練&検証データとする\n",
    "df[len_0_2:].to_csv(\"./train_eval.tsv\", sep='\\t', index=False, header=None)\n",
    "print(df[len_0_2:].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tsvファイルをPyTorchのtorchtextのDataLoaderに変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchtext==0.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 日本語BERT用のtokenizerを用意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext  # torchtextを使用\n",
    "from transformers.modeling_bert import BertModel\n",
    "from transformers.tokenization_bert_japanese import BertJapaneseTokenizer\n",
    "\n",
    "# 日本語BERTの分かち書き用tokenizerです\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(\n",
    "    'bert-base-japanese-whole-word-masking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データを読み込んだときに、読み込んだ内容に対して行う処理を定義\n",
    "\n",
    "max_length = 512  # 東北大学_日本語版の最大の単語数（サブワード数）は512\n",
    "\n",
    "\n",
    "def tokenizer_512(input_text):\n",
    "    \"\"\"torchtextのtokenizerとして扱えるように、512単語のpytorchでのencodeを定義。ここで[0]を指定し忘れないように\"\"\"\n",
    "    return tokenizer.encode(input_text, max_length=512, return_tensors='pt')[0]\n",
    "\n",
    "\n",
    "TEXT = torchtext.data.Field(sequential=True, tokenize=tokenizer_512, use_vocab=False, lower=False,\n",
    "                            include_lengths=True, batch_first=True, fix_length=max_length, pad_token=0)\n",
    "# 注意：tokenize=tokenizer.encodeと、.encodeをつけます。padding[PAD]のindexが0なので、0を指定します。\n",
    "\n",
    "LABEL = torchtext.data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "# (注釈)：各引数を再確認\n",
    "# sequential: データの長さが可変か？文章は長さがいろいろなのでTrue.ラベルはFalse\n",
    "# tokenize: 文章を読み込んだときに、前処理や単語分割をするための関数を定義\n",
    "# use_vocab：単語をボキャブラリーに追加するかどうか\n",
    "# lower：アルファベットがあったときに小文字に変換するかどうか\n",
    "# include_length: 文章の単語数のデータを保持するか\n",
    "# batch_first：ミニバッチの次元を用意するかどうか\n",
    "# fix_length：全部の文章をfix_lengthと同じ長さになるように、paddingします\n",
    "# init_token, eos_token, pad_token, unk_token：文頭、文末、padding、未知語に対して、どんな単語を与えるかを指定\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各tsvファイルを読み込み、分かち書きをしてdatasetにします\n",
    "# 少し時間がかかります\n",
    "# train_eval：5901個、test：1475個\n",
    "dataset_train_eval, dataset_test = torchtext.data.TabularDataset.splits(\n",
    "    path='.', train='train_eval.tsv', test='test.tsv', format='tsv', fields=[('Text', TEXT), ('Label', LABEL)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4426\n",
      "1475\n",
      "1475\n"
     ]
    }
   ],
   "source": [
    "# torchtext.data.Datasetのsplit関数で訓練データと検証データを分ける\n",
    "# train_eval：5901個、test：1475個\n",
    "\n",
    "import random\n",
    "\n",
    "dataset_train, dataset_eval = dataset_train_eval.split(\n",
    "    split_ratio=1.0 - 1475/5901, random_state=random.seed(1234))\n",
    "\n",
    "# datasetの長さを確認してみる\n",
    "print(dataset_train.__len__())\n",
    "print(dataset_eval.__len__())\n",
    "print(dataset_test.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    2,  1925,  3441,    23,    57,  3356,    24, 10271, 29170,     5,\n",
      "          454, 28611,     5,    36,  2236,  1421,    38,  6783,     5,    51,\n",
      "           12,     6,   186, 28843,     6,  5693, 28778,     5,  8408,  2502,\n",
      "           11,    15,    16,    33,     5,    14,    36,  2236,  1421,    84,\n",
      "           80,    38,  1568,     8,    36,  3250,     9,   703,  1540, 28796,\n",
      "         1540,    13,    15,    16,    21,    16,    28,     6,   393,    12,\n",
      "            9, 24994, 28461,    49, 13770,     7,   704,    11,  6840, 28468,\n",
      "           16,    33,    38,   140,  2309, 19309,  2716,    11, 10981,    16,\n",
      "           33,    53,    28,   707,   124,  2992,    14,    35,    35,    35,\n",
      "            8,  9147, 22637,     7,  4916,    11,    82,  1931,    12,    33,\n",
      "           13,     6, 20628,   145,     7,   881,    15,   790,     6,   239,\n",
      "        28781,    14, 17020,   790,    34,   947,   598, 29322, 28804,   679,\n",
      "         9147, 22637,     5,    36,  1703, 27258, 29001,    38,   559,    14,\n",
      "         6968,     5,  5419,    11,  4294,    34,   679, 28496, 11475,     9,\n",
      "         1658, 28454, 14657,     5, 21870,    38,    13, 24745, 28489, 28512,\n",
      "           15,    16,  9147, 22637,    11,  9709,    16,   546,    10,  1036,\n",
      "            6,    36,  2787,  2935, 23887,  2794,    14,    80,    38,    36,\n",
      "         1330,    16,  3488,     1,    14,    80,    38,    13,   872, 29390,\n",
      "        28506,    10,    45,     6,   130,  6769,  1058,    29,  2935, 15791,\n",
      "        26210,    49,   160,   410, 28444, 28662,    14,  2885,  3133,    12,\n",
      "            9,    80,  5602,     6,  2794,     5,   378,    14,  3597,  4697,\n",
      "           40,  8786,    16,    21,   790,     6,   160, 28652,    14, 25329,\n",
      "            5,  2794,    13, 13964,   332,    58,    16,    21,   790,    35,\n",
      "           35,    35,     8, 12272, 17840,     7,    36, 12959,  4916,    11,\n",
      "         1330,    16,    33,  5602,  1270,     7,  2250,  1549,  4916,    14,\n",
      "           80,    38,   140,     5,     9,     6,    36,  6638,    84,    80,\n",
      "           38,  1568,    14, 13574,  8994,    18,   469, 28564,     8,  3469,\n",
      "         2108,     7,  5675,   628,    10,  9147, 22637,     5, 17372,    11,\n",
      "         6788,  9719,  2078,    16,  8282,    13,    71,    16,  2501,    45,\n",
      "           14,   707,     5,    14,     6,   276,    19,   174,     7,  7109,\n",
      "           12,  9809,    10,  2869,   120,     5,  2794,    49,     1,    64,\n",
      "            6,    36,  2110,    16,    80, 18964, 28536,     6,  1750,  4541,\n",
      "         2435,    16,  6638,    84,    80,    38,  4916,   558,     8,   373,\n",
      "           36,  1703, 27258, 29001,    38,     5,  5591,   104,    50,   742,\n",
      "           62,   586,  1302,    23, 18364, 28504, 28447,    24,    35, 14650,\n",
      "            5,  7999,  2239, 13519, 28482,  2375,     7,  1940,   312,     6,\n",
      "         2419,     5, 15791, 26210,    11,  3804,  4596,   203,    16,    21,\n",
      "           80,  1080,  4494,    14,    36,  1750,  4541,  3721,    45,    38,\n",
      "           75,    13,    29,     8,    36,  1218,  4916,    11,  9809,    16,\n",
      "         1497,    13,     6, 21437,  1040,    11, 26731,    16,  1820,    16,\n",
      "         6481, 28506,  4541,    40,    63, 19553, 23887,  4830,    65,    13,\n",
      "         2367,  7071,  3566,    16,  7613,    16,  1497,     8,  1778,  5408,\n",
      "           16, 10123,    26,   191,    16,  1497,    45,  6172,     6,   854,\n",
      "           81,     7,    59, 26349,     5,  1273,    26,    11,  6007,    16,\n",
      "         1820,    16,    33,    45,     7,   139,  1058,  2992,     8, 11047,\n",
      "           18,     5,     9,     6, 26349,    12,  2787,   312,  1040,     5,\n",
      "          816,  1549,  2794,     5, 18447,    11,  2502,  3292, 11453,    45,\n",
      "            8,   319, 28727, 28484,    45,    12,     6,  7656, 18447,    18,\n",
      "          120,    11, 16941,  2755,    14, 26929,  2610,    15,     6,   375,\n",
      "            5,    53,     7,   816,    16,  8697,  3361,    11, 26349,     7,\n",
      "         3500,    45,     7,   139,     8,   218,    14,     6,    59, 26349,\n",
      "           11,     3])\n",
      "長さ： 512\n",
      "ラベル： 7\n"
     ]
    }
   ],
   "source": [
    "# datasetの中身を確認してみる\n",
    "item = next(iter(dataset_train))\n",
    "print(item.Text)\n",
    "print(\"長さ：\", len(item.Text))  # 長さを確認 [CLS]から始まり[SEP]で終わる。512より長いと後ろが切れる\n",
    "print(\"ラベル：\", item.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '写真', '一覧', '(', '4', '件', ')', '昨', '##今', 'の', '空', '##前', 'の', '「', '片', '##付け', '」', 'ブーム', 'の', '中', 'で', '、', '内', '##心', '、', '肩', '##身', 'の', '狭い', '思い', 'を', 'し', 'て', 'いる', 'の', 'が', '「', '片', '##付け', 'られ', 'ない', '」', '女子', '。', '「', '部屋', 'は', 'ゴ', '##チャ', '##ゴ', '##チャ', 'と', 'し', 'て', 'い', 'て', 'も', '、', '外', 'で', 'は', 'おしゃ', '##れ', 'や', 'メイク', 'に', '気', 'を', 'つか', '##っ', 'て', 'いる', '」', 'という', 'ギリ', '##ギリ', '##ライン', 'を', '保っ', 'て', 'いる', '人', 'も', '多い', 'よう', 'です', 'が', '・', '・', '・', '。', 'クロー', '##ゼット', 'に', 'モノ', 'を', 'ため', '##込ん', 'で', 'いる', 'と', '、', 'コーディネ', '##ート', 'に', '影響', 'し', 'たり', '、', '運', '##気', 'が', '下がっ', 'たり', 'する', 'ので', '要', '##注', '##意', '!', 'クロー', '##ゼット', 'の', '「', '断', '捨', '##離', '」', '度', 'が', 'あなた', 'の', 'ファッション', 'を', '左右', 'する', '!', '##「', '明日', 'は', '久', '##し', '##ぶり', 'の', 'デート', '」', 'と', 'うき', '##う', '##き', 'し', 'て', 'クロー', '##ゼット', 'を', '開け', 'て', 'み', 'た', 'ものの', '、', '「', 'あれ', '?', '着る', '服', 'が', 'ない', '」', '「', '持っ', 'て', '行く', '[UNK]', 'が', 'ない', '」', 'と', '青', '##ざ', '##め', 'た', 'こと', '、', 'あり', 'ませ', 'ん', 'か', '?', 'ワード', 'ローブ', 'や', '小', '物', '##の', '##数', 'が', '少ない', 'わけ', 'で', 'は', 'ない', 'のに', '、', '服', 'の', '多く', 'が', 'すでに', '流行', 'から', '外れ', 'て', 'い', 'たり', '、', '小', '##物', 'が', '手持ち', 'の', '服', 'と', '合わ', 'なく', 'なっ', 'て', 'い', 'たり', '・', '・', '・', '。', 'こんな', 'ふう', 'に', '「', 'たくさん', 'モノ', 'を', '持っ', 'て', 'いる', 'のに', '身', 'に', 'つけ', 'たい', 'モノ', 'が', 'ない', '」', 'という', 'の', 'は', '、', '「', '捨て', 'られ', 'ない', '」', '女子', 'が', '陥り', 'がち', 'な', 'ワ', '##ナ', '。', 'パン', '##パン', 'に', '詰', '##まっ', 'た', 'クロー', '##ゼット', 'の', '中身', 'を', '引っ', '##張り', '出し', 'て', 'みる', 'と', '出', 'て', 'くる', 'こと', 'が', '多い', 'の', 'が', '、', '数', '年', '前', 'に', 'ボーナス', 'で', '買っ', 'た', 'ブランド', 'もの', 'の', '服', 'や', '[UNK]', 'など', '、', '「', '使っ', 'て', 'ない', 'けれ', '##ど', '、', 'もっ', '##たい', '##なく', 'て', '捨て', 'られ', 'ない', '」', 'モノ', 'たち', '。', 'しかし', '「', '断', '捨', '##離', '」', 'の', '提唱', '者', 'として', '知ら', 'れる', 'クラ', 'スター', '(', 'がら', '##く', '##た', ')', '・', 'コンサルタント', 'の', 'やま', '##した', 'ひで', '##こ', 'さん', 'に', 'よれ', 'ば', '、', 'それら', 'の', 'ワード', 'ローブ', 'を', '有効', '活用', 'でき', 'て', 'い', 'ない', '状態', 'こそ', 'が', '「', 'もっ', '##たい', '##ない', 'こと', '」', 'だ', 'と', 'か', '。', '「', '高い', 'モノ', 'を', '買っ', 'て', 'しまう', 'と', '、', 'どうしても', '自分', 'を', '責め', 'て', 'しまっ', 'て', '後ろ', '##め', '##たい', 'から', '『', 'いつか', '着る', 'かも', '』', 'と', 'つい', '##つい', '取っ', 'て', 'おい', 'て', 'しまう', '。', 'そう', 'やっ', 'て', '堆積', 'さ', 'せ', 'て', 'しまう', 'こと', 'って', '、', '結果', '的', 'に', 'その', '洋服', 'の', '良', 'さ', 'を', '殺し', 'て', 'しまっ', 'て', 'いる', 'こと', 'に', 'なる', 'ん', 'です', '。', '大切', 'な', 'の', 'は', '、', '洋服', 'で', 'あれ', 'ば', '自分', 'の', '着', 'たい', '服', 'の', '旬', 'を', '思い', '##切り', '楽しむ', 'こと', '。', '手', '##放', '##す', 'こと', 'で', '、', '改めて', '旬', 'な', 'もの', 'を', '買う', 'エネルギー', 'が', '湧き', 'ます', 'し', '、', '他', 'の', '人', 'に', '着', 'て', 'もらう', '機会', 'を', '洋服', 'に', '与える', 'こと', 'に', 'なる', '。', 'それ', 'が', '、', 'その', '洋服', 'を', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'peachy'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# datasetの中身を文章に戻し、確認\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(item.Text.tolist()))  # 文章\n",
    "dic_id2cat[int(item.Label)]  # id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaderを作成します（torchtextの文脈では単純にiteraterと呼ばれています）\n",
    "batch_size = 16  # BERTでは16、32あたりを使用する\n",
    "\n",
    "dl_train = torchtext.data.Iterator(\n",
    "    dataset_train, batch_size=batch_size, train=True)\n",
    "\n",
    "dl_eval = torchtext.data.Iterator(\n",
    "    dataset_eval, batch_size=batch_size, train=False, sort=False)\n",
    "\n",
    "dl_test = torchtext.data.Iterator(\n",
    "    dataset_test, batch_size=batch_size, train=False, sort=False)\n",
    "\n",
    "# 辞書オブジェクトにまとめる\n",
    "dataloaders_dict = {\"train\": dl_train, \"val\": dl_eval}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 16]\n",
      "\t[.Text]:('[torch.LongTensor of size 16x512]', '[torch.LongTensor of size 16]')\n",
      "\t[.Label]:[torch.LongTensor of size 16]\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# DataLoaderの動作確認\n",
    "\n",
    "batch = next(iter(dl_test))\n",
    "print(batch)\n",
    "print(batch.Text[0].shape)\n",
    "print(batch.Label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. BERTのクラス分類用のモデルを用意する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers.modeling_bert import BertModel\n",
    "\n",
    "# BERTの日本語学習済みパラメータのモデルです\n",
    "model = BertModel.from_pretrained('bert-base-japanese-whole-word-masking')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class BertForLivedoor(nn.Module):\n",
    "    '''BERTモデルにLivedoorニュースの9クラスを判定する部分をつなげたモデル'''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BertForLivedoor, self).__init__()\n",
    "\n",
    "        # BERTモジュール\n",
    "        self.bert = model  # 日本語学習済みのBERTモデル\n",
    "\n",
    "        # headにクラス予測を追加\n",
    "        # 入力はBERTの出力特徴量の次元768、出力は9クラス\n",
    "        self.cls = nn.Linear(in_features=768, out_features=9)\n",
    "\n",
    "        # 重み初期化処理\n",
    "        nn.init.normal_(self.cls.weight, std=0.02)\n",
    "        nn.init.normal_(self.cls.bias, 0)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        '''\n",
    "        input_ids： [batch_size, sequence_length]の文章の単語IDの羅列\n",
    "        '''\n",
    "\n",
    "        # BERTの基本モデル部分の順伝搬\n",
    "        # 順伝搬させる\n",
    "        result = self.bert(input_ids)  # reult は、sequence_output, pooled_output\n",
    "\n",
    "        # sequence_outputの先頭の単語ベクトルを抜き出す\n",
    "        vec_0 = result[0]  # 最初の0がsequence_outputを示す\n",
    "        vec_0 = vec_0[:, 0, :]  # 全バッチ。先頭0番目の単語の全768要素\n",
    "        vec_0 = vec_0.view(-1, 768)  # sizeを[batch_size, hidden_size]に変換\n",
    "        output = self.cls(vec_0)  # 全結合層\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ネットワーク設定完了\n"
     ]
    }
   ],
   "source": [
    "# モデル構築\n",
    "net = BertForLivedoor()\n",
    "\n",
    "# 訓練モードに設定\n",
    "net.train()\n",
    "\n",
    "print('ネットワーク設定完了')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. ファインチューニングの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 勾配計算を最後のBertLayerモジュールと追加した分類アダプターのみ実行\n",
    "\n",
    "# 1. まず全部を、勾配計算Falseにしてしまう\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 2. BertLayerモジュールの最後を勾配計算ありに変更\n",
    "for param in net.bert.encoder.layer[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 3. 識別器を勾配計算ありに変更\n",
    "for param in net.cls.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適化手法の設定\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# BERTの元の部分はファインチューニング\n",
    "optimizer = optim.Adam([\n",
    "    {'params': net.bert.encoder.layer[-1].parameters(), 'lr': 5e-5},\n",
    "    {'params': net.cls.parameters(), 'lr': 1e-4}\n",
    "])\n",
    "\n",
    "# 損失関数の設定\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# nn.LogSoftmax()を計算してからnn.NLLLoss(negative log likelihood loss)を計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを学習させる関数を作成\n",
    "\n",
    "\n",
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "\n",
    "    # GPUが使えるかを確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"使用デバイス：\", device)\n",
    "    print('-----start-------')\n",
    "\n",
    "    # ネットワークをGPUへ\n",
    "    net.to(device)\n",
    "\n",
    "    # ネットワークがある程度固定であれば、高速化させる\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # ミニバッチのサイズ\n",
    "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
    "\n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs):\n",
    "        # epochごとの訓練と検証のループ\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()  # モデルを訓練モードに\n",
    "            else:\n",
    "                net.eval()   # モデルを検証モードに\n",
    "\n",
    "            epoch_loss = 0.0  # epochの損失和\n",
    "            epoch_corrects = 0  # epochの正解数\n",
    "            iteration = 1\n",
    "\n",
    "            # データローダーからミニバッチを取り出すループ\n",
    "            for batch in (dataloaders_dict[phase]):\n",
    "                # batchはTextとLableの辞書型変数\n",
    "\n",
    "                # GPUが使えるならGPUにデータを送る\n",
    "                inputs = batch.Text[0].to(device)  # 文章\n",
    "                labels = batch.Label.to(device)  # ラベル\n",
    "\n",
    "                # optimizerを初期化\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 順伝搬（forward）計算\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "\n",
    "                    # BERTに入力\n",
    "                    outputs = net(inputs)\n",
    "\n",
    "                    loss = criterion(outputs, labels)  # 損失を計算\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
    "\n",
    "                    # 訓練時はバックプロパゲーション\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
    "                            acc = (torch.sum(preds == labels.data)\n",
    "                                   ).double()/batch_size\n",
    "                            print('イテレーション {} || Loss: {:.4f} || 10iter. || 本イテレーションの正解率：{}'.format(\n",
    "                                iteration, loss.item(),  acc))\n",
    "\n",
    "                    iteration += 1\n",
    "\n",
    "                    # 損失と正解数の合計を更新\n",
    "                    epoch_loss += loss.item() * batch_size\n",
    "                    epoch_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            # epochごとのlossと正解率\n",
    "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "            epoch_acc = epoch_corrects.double(\n",
    "            ) / len(dataloaders_dict[phase].dataset)\n",
    "\n",
    "            print('Epoch {}/{} | {:^5} |  Loss: {:.4f} Acc: {:.4f}'.format(epoch+1, num_epochs,\n",
    "                                                                           phase, epoch_loss, epoch_acc))\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用デバイス： cuda:0\n",
      "-----start-------\n",
      "イテレーション 10 || Loss: 2.0561 || 10iter. || 本イテレーションの正解率：0.25\n",
      "イテレーション 20 || Loss: 1.9687 || 10iter. || 本イテレーションの正解率：0.3125\n",
      "イテレーション 30 || Loss: 1.6725 || 10iter. || 本イテレーションの正解率：0.4375\n",
      "イテレーション 40 || Loss: 1.4457 || 10iter. || 本イテレーションの正解率：0.6875\n",
      "イテレーション 50 || Loss: 1.6102 || 10iter. || 本イテレーションの正解率：0.5625\n",
      "イテレーション 60 || Loss: 1.1687 || 10iter. || 本イテレーションの正解率：0.625\n",
      "イテレーション 70 || Loss: 1.1155 || 10iter. || 本イテレーションの正解率：0.625\n",
      "イテレーション 80 || Loss: 1.3749 || 10iter. || 本イテレーションの正解率：0.5625\n",
      "イテレーション 90 || Loss: 0.6669 || 10iter. || 本イテレーションの正解率：0.75\n",
      "イテレーション 100 || Loss: 0.7625 || 10iter. || 本イテレーションの正解率：0.75\n",
      "イテレーション 110 || Loss: 0.5884 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 120 || Loss: 0.4101 || 10iter. || 本イテレーションの正解率：0.8125\n",
      "イテレーション 130 || Loss: 0.8097 || 10iter. || 本イテレーションの正解率：0.6875\n",
      "イテレーション 140 || Loss: 0.5726 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 150 || Loss: 0.2804 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 160 || Loss: 0.8610 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 170 || Loss: 1.0727 || 10iter. || 本イテレーションの正解率：0.5625\n",
      "イテレーション 180 || Loss: 0.6986 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 190 || Loss: 0.3475 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 200 || Loss: 0.7739 || 10iter. || 本イテレーションの正解率：0.75\n",
      "イテレーション 210 || Loss: 0.4030 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 220 || Loss: 0.3220 || 10iter. || 本イテレーションの正解率：0.8125\n",
      "イテレーション 230 || Loss: 0.7151 || 10iter. || 本イテレーションの正解率：0.8125\n",
      "イテレーション 240 || Loss: 0.1705 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 250 || Loss: 0.3955 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 260 || Loss: 0.5617 || 10iter. || 本イテレーションの正解率：0.8125\n",
      "イテレーション 270 || Loss: 0.4554 || 10iter. || 本イテレーションの正解率：0.8125\n",
      "Epoch 1/5 | train |  Loss: 0.8546 Acc: 0.7286\n",
      "Epoch 1/5 |  val  |  Loss: 0.3839 Acc: 0.8664\n",
      "イテレーション 10 || Loss: 0.1316 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 20 || Loss: 0.4375 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 30 || Loss: 0.4868 || 10iter. || 本イテレーションの正解率：0.8125\n",
      "イテレーション 40 || Loss: 0.2388 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 50 || Loss: 0.2776 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 60 || Loss: 0.5640 || 10iter. || 本イテレーションの正解率：0.6875\n",
      "イテレーション 70 || Loss: 0.3134 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 80 || Loss: 0.3020 || 10iter. || 本イテレーションの正解率：0.8125\n",
      "イテレーション 90 || Loss: 0.5992 || 10iter. || 本イテレーションの正解率：0.8125\n",
      "イテレーション 100 || Loss: 0.6605 || 10iter. || 本イテレーションの正解率：0.8125\n",
      "イテレーション 110 || Loss: 0.3055 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 120 || Loss: 0.2739 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 130 || Loss: 0.4810 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 140 || Loss: 0.3221 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 150 || Loss: 0.6498 || 10iter. || 本イテレーションの正解率：0.75\n",
      "イテレーション 160 || Loss: 0.4333 || 10iter. || 本イテレーションの正解率：0.75\n",
      "イテレーション 170 || Loss: 0.4913 || 10iter. || 本イテレーションの正解率：0.75\n",
      "イテレーション 180 || Loss: 0.6587 || 10iter. || 本イテレーションの正解率：0.8125\n",
      "イテレーション 190 || Loss: 0.3676 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 200 || Loss: 0.1685 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 210 || Loss: 0.1484 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 220 || Loss: 0.5227 || 10iter. || 本イテレーションの正解率：0.8125\n",
      "イテレーション 230 || Loss: 0.0668 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 240 || Loss: 0.1380 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 250 || Loss: 0.2146 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 260 || Loss: 0.1349 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 270 || Loss: 0.1271 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "Epoch 2/5 | train |  Loss: 0.3402 Acc: 0.8882\n",
      "Epoch 2/5 |  val  |  Loss: 0.2943 Acc: 0.9064\n",
      "イテレーション 10 || Loss: 0.2167 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 20 || Loss: 0.6240 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 30 || Loss: 0.1195 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 40 || Loss: 0.3145 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 50 || Loss: 0.2092 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 60 || Loss: 0.1268 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 70 || Loss: 0.2971 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 80 || Loss: 0.2615 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 90 || Loss: 0.1447 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 100 || Loss: 0.6047 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 110 || Loss: 0.2140 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 120 || Loss: 0.3787 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 130 || Loss: 0.3643 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 140 || Loss: 0.0460 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 150 || Loss: 0.4994 || 10iter. || 本イテレーションの正解率：0.8125\n",
      "イテレーション 160 || Loss: 0.3478 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 170 || Loss: 0.0844 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 180 || Loss: 0.1489 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 190 || Loss: 0.4320 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 200 || Loss: 0.1729 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 210 || Loss: 0.2688 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 220 || Loss: 0.3070 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 230 || Loss: 0.1649 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 240 || Loss: 0.1690 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 250 || Loss: 0.1794 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 260 || Loss: 0.1407 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 270 || Loss: 0.4087 || 10iter. || 本イテレーションの正解率：0.8125\n",
      "Epoch 3/5 | train |  Loss: 0.2274 Acc: 0.9275\n",
      "Epoch 3/5 |  val  |  Loss: 0.2930 Acc: 0.9078\n",
      "イテレーション 10 || Loss: 0.1216 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 20 || Loss: 0.1556 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 30 || Loss: 0.1456 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 40 || Loss: 0.1314 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 50 || Loss: 0.0896 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 60 || Loss: 0.0437 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 70 || Loss: 0.1089 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 80 || Loss: 0.0469 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 90 || Loss: 0.3088 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 100 || Loss: 0.1189 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 110 || Loss: 0.0816 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 120 || Loss: 0.1235 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 130 || Loss: 0.0255 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 140 || Loss: 0.0644 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 150 || Loss: 0.1122 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 160 || Loss: 0.0344 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 170 || Loss: 0.1866 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 180 || Loss: 0.1469 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 190 || Loss: 0.0842 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 200 || Loss: 0.0549 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 210 || Loss: 0.1410 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 220 || Loss: 0.0274 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 230 || Loss: 0.4119 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 240 || Loss: 0.1203 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 250 || Loss: 0.0236 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 260 || Loss: 0.3184 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 270 || Loss: 0.0689 || 10iter. || 本イテレーションの正解率：1.0\n",
      "Epoch 4/5 | train |  Loss: 0.1678 Acc: 0.9458\n",
      "Epoch 4/5 |  val  |  Loss: 0.2671 Acc: 0.9207\n",
      "イテレーション 10 || Loss: 0.0633 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 20 || Loss: 0.0256 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 30 || Loss: 0.0868 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 40 || Loss: 0.1339 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 50 || Loss: 0.0200 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 60 || Loss: 0.3601 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 70 || Loss: 0.1832 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 80 || Loss: 0.1308 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 90 || Loss: 0.0141 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 100 || Loss: 0.0500 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 110 || Loss: 0.2191 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 120 || Loss: 0.3589 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 130 || Loss: 0.0692 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 140 || Loss: 0.0562 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 150 || Loss: 0.0881 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 160 || Loss: 0.0296 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 170 || Loss: 0.0277 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 180 || Loss: 0.0688 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 190 || Loss: 0.0553 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 200 || Loss: 0.0372 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 210 || Loss: 0.0224 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 220 || Loss: 0.2596 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 230 || Loss: 0.1074 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 240 || Loss: 0.1057 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 250 || Loss: 0.2273 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 260 || Loss: 0.0166 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 270 || Loss: 0.0945 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "Epoch 5/5 | train |  Loss: 0.1066 Acc: 0.9679\n",
      "Epoch 5/5 |  val  |  Loss: 0.2776 Acc: 0.9281\n"
     ]
    }
   ],
   "source": [
    "# 学習・検証を実行する。1epochに2分ほどかかる\n",
    "num_epochs = 5\n",
    "net_trained = train_model(net, dataloaders_dict,\n",
    "                          criterion, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93/93 [00:45<00:00,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "テストデータ1475個での正解率：0.9254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# テストデータでの正解率を求める\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net_trained.eval()   # モデルを検証モードに\n",
    "net_trained.to(device)  # GPUが使えるならGPUへ送る\n",
    "\n",
    "# epochの正解数を記録する変数\n",
    "epoch_corrects = 0\n",
    "\n",
    "for batch in tqdm(dl_test):  # testデータのDataLoader\n",
    "    # batchはTextとLableの辞書オブジェクト\n",
    "    # GPUが使えるならGPUにデータを送る\n",
    "    inputs = batch.Text[0].to(device)  # 文章\n",
    "    labels = batch.Label.to(device)  # ラベル\n",
    "\n",
    "    # 順伝搬（forward）計算\n",
    "    with torch.set_grad_enabled(False):\n",
    "\n",
    "        # BertForLivedoorに入力\n",
    "        outputs = net_trained(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)  # 損失を計算\n",
    "        _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
    "        epoch_corrects += torch.sum(preds == labels.data)  # 正解数の合計を更新\n",
    "\n",
    "# 正解率\n",
    "epoch_acc = epoch_corrects.double() / len(dl_test.dataset)\n",
    "\n",
    "print('テストデータ{}個での正解率：{:.4f}'.format(len(dl_test.dataset), epoch_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正解率92.6%で9クラスの記事を分類できるようになった"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
